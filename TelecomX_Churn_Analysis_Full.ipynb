{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332a4334",
   "metadata": {},
   "source": [
    "# Análise e Modelagem de Churn de Clientes - TelecomX\n",
    "\n",
    "Este notebook apresenta uma análise completa do churn de clientes da TelecomX, desde o pré-processamento dos dados até a modelagem preditiva, avaliação de modelos e análise de importância de variáveis. O objetivo é identificar os principais fatores que levam os clientes a cancelar seus serviços e propor estratégias de retenção.\n",
    "\n",
    "**Conteúdo:**\n",
    "1.  Carregamento e Pré-processamento dos Dados\n",
    "2.  Análise Exploratória de Dados (EDA) e Visualizações\n",
    "3.  Modelagem Preditiva e Avaliação de Modelos\n",
    "4.  Análise de Importância das Variáveis e Comparação de Modelos\n",
    "5.  Conclusões e Estratégias de Retenção\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc445c",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Pré-processamento dos Dados\n",
    "\n",
    "Nesta seção, os dados são carregados, as colunas aninhadas são desaninhadas, colunas irrelevantes são removidas, valores ausentes são tratados e variáveis categóricas são transformadas em formato numérico utilizando One-Hot Encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "# Carregar os dados\n",
    "try:\n",
    "    df = pd.read_json(\"TelecomX_Data.json\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo TelecomX_Data.json não foi encontrado. Por favor, faça o upload do arquivo para o ambiente do Colab.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Dados originais carregados:\")\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Desaninhamento das colunas de dicionário\n",
    "def flatten_dict_column(df, column_name):\n",
    "    flattened_data = pd.json_normalize(df[column_name])\n",
    "    flattened_data.columns = [f\"{column_name}.{sub_col}\" for sub_col in flattened_data.columns]\n",
    "    df = df.drop(columns=[column_name]).join(flattened_data)\n",
    "    return df\n",
    "\n",
    "df = flatten_dict_column(df, 'customer')\n",
    "df = flatten_dict_column(df, 'phone')\n",
    "df = flatten_dict_column(df, 'internet')\n",
    "df = flatten_dict_column(df, 'account')\n",
    "\n",
    "# Eliminar colunas que não trazem valor (ID do cliente)\n",
    "if 'customerID' in df.columns:\n",
    "    df = df.drop(\"customerID\", axis=1)\n",
    "\n",
    "print(\"\n",
    "Dados após desaninhamento (primeiras 5 linhas e info):\")\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Tratar valores ausentes (se houver) e converter tipos\n",
    "df['account.Charges.Total'] = pd.to_numeric(df['account.Charges.Total'], errors='coerce')\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "\n",
    "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Transformar a variável target 'Churn' para numérica (0 e 1)\n",
    "if 'Churn' in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df['Churn'] = le.fit_transform(df['Churn'])\n",
    "\n",
    "# Converter colunas binárias de 'Yes'/'No' para 1/0\n",
    "binary_cols_to_convert = [\n",
    "    'customer.Partner', 'customer.Dependents', 'phone.PhoneService', 'phone.MultipleLines',\n",
    "    'internet.OnlineSecurity', 'internet.OnlineBackup', 'internet.DeviceProtection',\n",
    "    'internet.TechSupport', 'internet.StreamingTV', 'internet.StreamingMovies',\n",
    "    'account.PaperlessBilling', 'customer.gender'\n",
    "]\n",
    "\n",
    "for col in binary_cols_to_convert:\n",
    "    if col in df.columns and df[col].dtype == 'object':\n",
    "        df[col] = df[col].apply(lambda x: 1 if x == 'Yes' else (0 if x == 'No' else (1 if x == 'Male' else 0)))\n",
    "\n",
    "# Re-identificar colunas categóricas após o tratamento das binárias e imputação\n",
    "final_categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Aplicar OneHotEncoder para as colunas categóricas restantes\n",
    "if final_categorical_cols:\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), final_categorical_cols)\n",
    "        ], remainder='passthrough'\n",
    "    )\n",
    "    df_processed = preprocessor.fit_transform(df)\n",
    "    ohe_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(final_categorical_cols)\n",
    "    \n",
    "    passthrough_cols = [col for col in df.columns if col not in final_categorical_cols]\n",
    "    \n",
    "    df = pd.DataFrame(df_processed, columns=list(ohe_feature_names) + passthrough_cols)\n",
    "\n",
    "print(\"\n",
    "Dados após pré-processamento completo (primeiras 5 linhas e info):\")\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Salvar o dataframe pré-processado para uso posterior\n",
    "df.to_csv(\"telecomx_processed_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105e17ca",
   "metadata": {},
   "source": [
    "## 2. Análise Exploratória de Dados (EDA) e Visualizações\n",
    "\n",
    "Nesta seção, realizamos uma análise exploratória para entender a distribuição das variáveis, identificar padrões e relações, e visualizar a proporção de churn, correlações entre variáveis e o impacto de fatores como tempo de contrato e gastos totais no churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8036f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregar os dados pré-processados\n",
    "try:\n",
    "    df = pd.read_csv(\"telecomx_processed_data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo telecomx_processed_data.csv não foi encontrado. Execute a etapa de pré-processamento primeiro.\")\n",
    "    exit()\n",
    "\n",
    "# 1. Proporção de clientes que evadiram (Churn)\n",
    "churn_counts = df[\"Churn\"].value_counts(normalize=True)\n",
    "print(\"\n",
    "Proporção de Churn (0=Não, 1=Sim):\")\n",
    "print(churn_counts)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.barplot(x=churn_counts.index, y=churn_counts.values, palette=\"viridis\")\n",
    "plt.title(\"Proporção de Clientes com Churn\")\n",
    "plt.xlabel(\"Churn (0: Não, 1: Sim)\")\n",
    "plt.ylabel(\"Proporção\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Não Churn\", \"Churn\"])\n",
    "plt.show()\n",
    "\n",
    "# 2. Matriz de Correlação\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(df.corr(), annot=False, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Matriz de Correlação das Variáveis\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Relação entre Tempo de contrato (customer.tenure) e Churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x=\"Churn\", y=\"customer.tenure\", palette=\"pastel\")\n",
    "plt.title(\"Tempo de Contrato vs. Churn\")\n",
    "plt.xlabel(\"Churn (0: Não, 1: Sim)\")\n",
    "plt.ylabel(\"Tempo de Contrato (meses)\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Não Churn\", \"Churn\"])\n",
    "plt.show()\n",
    "\n",
    "# 4. Relação entre Total gasto (account.Charges.Total) e Churn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x=\"Churn\", y=\"account.Charges.Total\", palette=\"pastel\")\n",
    "plt.title(\"Total Gasto vs. Churn\")\n",
    "plt.xlabel(\"Churn (0: Não, 1: Sim)\")\n",
    "plt.ylabel(\"Total Gasto\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Não Churn\", \"Churn\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50048f",
   "metadata": {},
   "source": [
    "## 3. Modelagem Preditiva e Avaliação de Modelos\n",
    "\n",
    "Nesta seção, dividimos os dados em conjuntos de treino e teste, treinamos múltiplos modelos de classificação (Regressão Logística, Árvore de Decisão, Random Forest, KNN, SVM) e avaliamos seu desempenho usando métricas como Acurácia, Precisão, Recall, F1-score e Matriz de Confusão.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Carregar os dados pré-processados\n",
    "try:\n",
    "    df = pd.read_csv(\"telecomx_processed_data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo telecomx_processed_data.csv não foi encontrado. Execute a etapa de pré-processamento primeiro.\")\n",
    "    exit()\n",
    "\n",
    "# Definir X (features) e y (target)\n",
    "X = df.drop(\"Churn\", axis=1)\n",
    "y = df[\"Churn\"]\n",
    "\n",
    "# Remapear Churn para 0 e 1, tratando valores inesperados\n",
    "y = y[y.isin([1.0, 2.0])]\n",
    "X = X.loc[y.index]\n",
    "y = y.map({1.0: 0, 2.0: 1})\n",
    "\n",
    "# Divisão dos dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Modelos a serem testados\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, solver='liblinear', max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Normalização/Padronização dos dados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Treinamento e avaliação dos modelos\n",
    "for name, model in models.items():\n",
    "    print(f\"\n",
    "--- Treinando e Avaliando: {name} ---\")\n",
    "    \n",
    "    if name in [\"Logistic Regression\", \"KNN\", \"SVM\"]:\n",
    "        X_train_model = X_train_scaled\n",
    "        X_test_model = X_test_scaled\n",
    "        print(f\"Dados normalizados usados para {name}.\")\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "        print(f\"Dados não normalizados usados para {name}.\")\n",
    "\n",
    "    model.fit(X_train_model, y_train)\n",
    "    y_pred = model.predict(X_test_model)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": conf_mat,\n",
    "        \"classification_report\": class_report\n",
    "    }\n",
    "\n",
    "    print(f\"Acurácia: {accuracy:.4f}\")\n",
    "    print(f\"Precisão: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"Matriz de Confusão:\n",
    "\", conf_mat)\n",
    "    print(\"Relatório de Classificação:\n",
    "\", class_report)\n",
    "\n",
    "# Salvar resultados para análise posterior\n",
    "import json\n",
    "with open(\"model_results.json\", \"w\") as f:\n",
    "    serializable_results = {}\n",
    "    for name, metrics in results.items():\n",
    "        serializable_results[name] = {\n",
    "            \"accuracy\": metrics[\"accuracy\"],\n",
    "            \"precision\": metrics[\"precision\"],\n",
    "            \"recall\": metrics[\"recall\"],\n",
    "            \"f1_score\": metrics[\"f1_score\"],\n",
    "            \"confusion_matrix\": metrics[\"confusion_matrix\"].tolist(),\n",
    "            \"classification_report\": metrics[\"classification_report\"]\n",
    "        }\n",
    "    json.dump(serializable_results, f, indent=4)\n",
    "\n",
    "print(\"Resultados dos modelos salvos em model_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1e35c",
   "metadata": {},
   "source": [
    "## 4. Análise de Importância das Variáveis e Comparação de Modelos\n",
    "\n",
    "Nesta seção, analisamos a importância das variáveis para os modelos treinados e realizamos uma comparação crítica entre eles, discutindo seus pontos fortes, fracos e a necessidade de normalização.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0832241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Carregar os dados pré-processados\n",
    "try:\n",
    "    df = pd.read_csv(\"telecomx_processed_data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo telecomx_processed_data.csv não foi encontrado. Execute a etapa de pré-processamento primeiro.\")\n",
    "    exit()\n",
    "\n",
    "# Definir X (features) e y (target)\n",
    "X = df.drop(\"Churn\", axis=1)\n",
    "y = df[\"Churn\"]\n",
    "\n",
    "# Remapear Churn para 0 e 1, tratando valores inesperados\n",
    "y = y[y.isin([1.0, 2.0])]\n",
    "X = X.loc[y.index]\n",
    "y = y.map({1.0: 0, 2.0: 1})\n",
    "\n",
    "# Divisão dos dados em treino e teste (re-executar para garantir consistência)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Normalização/Padronização dos dados (re-executar para garantir consistência)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Re-treinar modelos para acessar atributos de importância de features\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, solver='liblinear', max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "# Análise de Importância das Variáveis\n",
    "print(\"\n",
    "--- Análise de Importância das Variáveis ---\")\n",
    "\n",
    "# Logistic Regression\n",
    "model_lr = models[\"Logistic Regression\"]\n",
    "model_lr.fit(X_train_scaled, y_train)\n",
    "lr_coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': model_lr.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "print(\"\n",
    "Coeficientes da Regressão Logística (top 10):\n",
    "\", lr_coefficients.head(10))\n",
    "print(\"\n",
    "Coeficientes da Regressão Logística (bottom 10):\n",
    "\", lr_coefficients.tail(10))\n",
    "\n",
    "# Random Forest\n",
    "model_rf = models[\"Random Forest\"]\n",
    "model_rf.fit(X_train, y_train)\n",
    "rf_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': model_rf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\n",
    "Importância das Features do Random Forest (top 10):\n",
    "\", rf_importances.head(10))\n",
    "\n",
    "# Visualização da importância das features do Random Forest\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=rf_importances.head(10), palette='viridis')\n",
    "plt.title('Top 10 Features Mais Importantes (Random Forest)')\n",
    "plt.xlabel('Importância')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "# Comparação e Análise Crítica dos Modelos (carregar resultados salvos)\n",
    "try:\n",
    "    with open(\"model_results.json\", \"r\") as f:\n",
    "        results = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo model_results.json não foi encontrado. Execute a etapa de treinamento e avaliação primeiro.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\n",
    "--- Análise Crítica e Comparação dos Modelos ---\")\n",
    "\n",
    "best_model = None\n",
    "best_f1 = -1\n",
    "\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\n",
    "Modelo: {name}\")\n",
    "    print(f\"  Acurácia: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precisão: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-score: {metrics['f1_score']:.4f}\")\n",
    "    print(\"  Matriz de Confusão:\n",
    "\", np.array(metrics['confusion_matrix']))\n",
    "    print(\"  Relatório de Classificação:\n",
    "\", metrics['classification_report'])\n",
    "\n",
    "    if metrics['f1_score'] > best_f1:\n",
    "        best_f1 = metrics['f1_score']\n",
    "        best_model = name\n",
    "\n",
    "print(f\"\n",
    "O modelo com melhor desempenho (F1-score) foi: {best_model}\")\n",
    "\n",
    "print(\"\n",
    "Análise de Overfitting/Underfitting e Justificativas:\")\n",
    "print(\"\n",
    "- **Regressão Logística:** Teve um bom desempenho geral, especialmente em precisão para a classe majoritária. Por ser um modelo linear, é menos propenso a overfitting, mas pode ter underfitting se as relações nos dados forem muito complexas. A normalização foi crucial para este modelo.\")\n",
    "print(\"- **Árvore de Decisão:** Geralmente mais propensa a overfitting se não for controlada (profundidade máxima, etc.). Seu desempenho foi o mais baixo, indicando possível underfitting ou que o modelo é muito simples para capturar as nuances dos dados, ou overfitting nos dados de treino. Não requer normalização.\")\n",
    "print(\"- **Random Forest:** Um ensemble de árvores de decisão, geralmente mais robusto contra overfitting do que uma única árvore. Apresentou bom desempenho, ligeiramente inferior à Regressão Logística e SVM em F1-score, mas com boa precisão. Não requer normalização.\")\n",
    "print(\"- **KNN:** Sensível à escala dos dados, por isso a normalização foi aplicada. Seu desempenho foi intermediário. Pode ser sensível ao número de vizinhos (k) e à dimensionalidade dos dados.\")\n",
    "print(\"- **SVM:** Teve um desempenho similar ou ligeiramente superior à Regressão Logística em F1-score. Modelos SVM podem ser muito poderosos, mas a escolha do kernel e a otimização dos hiperparâmetros são cruciais. A normalização é essencial para o SVM.\")\n",
    "\n",
    "print(\"\n",
    "Considerações sobre Overfitting/Underfitting:\")\n",
    "print(\"Para avaliar overfitting/underfitting de forma mais robusta, seria necessário comparar as métricas de desempenho nos conjuntos de treino e teste. Se o desempenho no treino for significativamente melhor que no teste, há indícios de overfitting. Se o desempenho for baixo em ambos, há indícios de underfitting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf4b56",
   "metadata": {},
   "source": [
    "## 5. Conclusões e Estratégias de Retenção\n",
    "\n",
    "Com base nas análises realizadas, identificamos os principais fatores que contribuem para o churn de clientes e propomos estratégias de retenção acionáveis para a TelecomX.\n",
    "\n",
    "### Principais Fatores de Churn:\n",
    "\n",
    "*   **Tempo de Contrato (customer.tenure)**: Clientes com menor tempo de permanência têm maior probabilidade de churn.\n",
    "*   **Cobrança Total (account.Charges.Total)**: Valores totais gastos impactam significativamente a decisão de permanência.\n",
    "*   **Tipo de Contrato (account.Contract_Month-to-month)**: Contratos mensais apresentam maior risco de churn.\n",
    "*   **Tipo de Internet (internet.InternetService_Fiber optic)**: Clientes com fibra ótica mostram padrões específicos de churn.\n",
    "*   **Método de Pagamento (account.PaymentMethod_Electronic check)**: Pagamentos por cheque eletrônico correlacionam com maior churn.\n",
    "\n",
    "### Estratégias de Retenção Recomendadas:\n",
    "\n",
    "1.  **Programa de Onboarding Aprimorado**: Focar nos primeiros meses de relacionamento com acompanhamento proativo e suporte dedicado.\n",
    "2.  **Incentivos para Contratos de Longo Prazo**: Oferecer descontos e benefícios para clientes que optam por contratos anuais ou bianuais.\n",
    "3.  **Segmentação e Personalização**: Criar estratégias diferenciadas baseadas no perfil de gasto do cliente.\n",
    "4.  **Melhoria na Experiência de Pagamento**: Incentivar métodos de pagamento automáticos e simplificar processos.\n",
    "5.  **Otimização de Serviços de Internet**: Monitorar a qualidade do serviço e oferecer suporte especializado para clientes de fibra ótica.\n",
    "6.  **Sistema de Alerta Precoce**: Implementar um sistema de scoring de churn para identificar clientes em risco em tempo real e acionar campanhas de retenção proativas.\n",
    "\n",
    "Essas estratégias, se implementadas de forma coordenada, podem reduzir significativamente a taxa de churn e aumentar a satisfação do cliente na TelecomX.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
